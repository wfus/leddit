{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:06:21.014349Z",
     "start_time": "2020-03-15T07:06:04.933643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:06:12.366126 140098058590016 file_utils.py:38] PyTorch version 1.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from aita.AITAPredictor import AITAClassifier\n",
    "from aita.test_reader import AITATestReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.models.archival import load_archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-15T07:07:11.517Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:07:11.526926 140098058590016 archival.py:152] loading archive file /tmp/roberta-base.tar.gz\n",
      "I0315 00:07:11.545274 140098058590016 archival.py:161] extracting archive file /tmp/roberta-base.tar.gz to temp dir /tmp/tmpfweivbda\n",
      "I0315 00:07:20.286480 140098058590016 params.py:250] type = from_instances\n",
      "I0315 00:07:20.288161 140098058590016 vocabulary.py:293] Loading token dictionary from /tmp/tmpfweivbda/vocabulary.\n",
      "I0315 00:07:20.291813 140098058590016 params.py:250] model.type = basic_classifier\n",
      "I0315 00:07:20.293468 140098058590016 params.py:250] model.regularizer = None\n",
      "I0315 00:07:20.295173 140098058590016 params.py:250] model.text_field_embedder.type = basic\n",
      "I0315 00:07:20.297091 140098058590016 params.py:250] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer\n",
      "I0315 00:07:20.298882 140098058590016 params.py:250] model.text_field_embedder.token_embedders.tokens.model_name = roberta-base\n",
      "I0315 00:07:20.300225 140098058590016 params.py:250] model.text_field_embedder.token_embedders.tokens.max_length = 128\n",
      "I0315 00:07:20.734044 140098058590016 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0315 00:07:20.736681 140098058590016 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0315 00:07:21.312346 140098058590016 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /home/wfu/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0315 00:07:37.006964 140098058590016 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0315 00:07:37.011141 140098058590016 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0315 00:07:37.836335 140098058590016 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/wfu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0315 00:07:37.838081 140098058590016 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/wfu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0315 00:07:37.981927 140098058590016 params.py:250] model.seq2vec_encoder.type = cls_pooler\n",
      "I0315 00:07:37.984321 140098058590016 params.py:250] model.seq2vec_encoder.embedding_dim = 768\n",
      "I0315 00:07:37.985726 140098058590016 params.py:250] model.seq2vec_encoder.cls_is_last_token = False\n",
      "I0315 00:07:37.987330 140098058590016 params.py:250] model.seq2seq_encoder = None\n",
      "I0315 00:07:37.990327 140098058590016 params.py:250] model.feedforward.input_dim = 768\n",
      "I0315 00:07:37.992414 140098058590016 params.py:250] model.feedforward.num_layers = 1\n",
      "I0315 00:07:37.993797 140098058590016 params.py:250] model.feedforward.hidden_dims = 768\n",
      "I0315 00:07:37.995489 140098058590016 params.py:250] model.feedforward.activations = tanh\n",
      "I0315 00:07:37.998195 140098058590016 params.py:250] model.feedforward.dropout = 0.0\n",
      "I0315 00:07:38.011882 140098058590016 params.py:250] model.dropout = 0.1\n",
      "I0315 00:07:38.013907 140098058590016 params.py:250] model.num_labels = None\n",
      "I0315 00:07:38.015289 140098058590016 params.py:250] model.label_namespace = labels\n",
      "I0315 00:07:38.016842 140098058590016 params.py:250] model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f6a66378588>\n",
      "I0315 00:07:38.020827 140098058590016 initializers.py:410] Initializing parameters\n",
      "I0315 00:07:38.023374 140098058590016 initializers.py:429] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0315 00:07:38.024672 140098058590016 initializers.py:435]    _classification_layer.bias\n",
      "I0315 00:07:38.025867 140098058590016 initializers.py:435]    _classification_layer.weight\n",
      "I0315 00:07:38.027120 140098058590016 initializers.py:435]    _feedforward._linear_layers.0.bias\n",
      "I0315 00:07:38.028414 140098058590016 initializers.py:435]    _feedforward._linear_layers.0.weight\n",
      "I0315 00:07:38.031800 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias\n",
      "I0315 00:07:38.032998 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight\n",
      "I0315 00:07:38.034188 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight\n",
      "I0315 00:07:38.035429 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0315 00:07:38.037062 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight\n",
      "I0315 00:07:38.038332 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.039520 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.040454 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0315 00:07:38.041351 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:07:38.042443 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0315 00:07:38.043523 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0315 00:07:38.044700 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0315 00:07:38.045891 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0315 00:07:38.047055 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0315 00:07:38.048132 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0315 00:07:38.049465 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0315 00:07:38.050802 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0315 00:07:38.052081 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0315 00:07:38.053258 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0315 00:07:38.054514 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0315 00:07:38.055703 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0315 00:07:38.056755 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.057752 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.058781 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0315 00:07:38.059927 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0315 00:07:38.061060 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0315 00:07:38.062109 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0315 00:07:38.063294 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0315 00:07:38.064313 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0315 00:07:38.065371 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0315 00:07:38.066364 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0315 00:07:38.067395 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0315 00:07:38.068416 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0315 00:07:38.069428 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0315 00:07:38.070426 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0315 00:07:38.071434 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0315 00:07:38.072408 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0315 00:07:38.073410 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.074441 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.075585 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0315 00:07:38.076675 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0315 00:07:38.077760 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0315 00:07:38.078744 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0315 00:07:38.079834 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0315 00:07:38.080780 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0315 00:07:38.081784 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0315 00:07:38.082831 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0315 00:07:38.083905 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "I0315 00:07:38.084846 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0315 00:07:38.085813 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0315 00:07:38.086786 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0315 00:07:38.087863 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0315 00:07:38.088823 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0315 00:07:38.089900 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.090863 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.091798 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0315 00:07:38.092712 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0315 00:07:38.093909 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0315 00:07:38.095026 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:07:38.096206 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0315 00:07:38.097479 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0315 00:07:38.098604 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0315 00:07:38.099669 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0315 00:07:38.100814 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0315 00:07:38.101933 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0315 00:07:38.103053 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0315 00:07:38.104130 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0315 00:07:38.105317 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0315 00:07:38.106424 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0315 00:07:38.107506 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.108642 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.109782 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0315 00:07:38.110865 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0315 00:07:38.111954 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0315 00:07:38.112957 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0315 00:07:38.113972 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0315 00:07:38.114984 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0315 00:07:38.116025 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0315 00:07:38.117008 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0315 00:07:38.118022 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0315 00:07:38.119033 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0315 00:07:38.119930 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0315 00:07:38.120820 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0315 00:07:38.121716 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0315 00:07:38.122682 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0315 00:07:38.133822 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.134833 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.135766 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0315 00:07:38.136650 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0315 00:07:38.137563 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0315 00:07:38.138578 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0315 00:07:38.139488 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0315 00:07:38.140360 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0315 00:07:38.141260 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0315 00:07:38.142240 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0315 00:07:38.143290 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0315 00:07:38.144275 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0315 00:07:38.158098 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0315 00:07:38.159000 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0315 00:07:38.159918 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0315 00:07:38.160804 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0315 00:07:38.161716 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.162611 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.163513 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0315 00:07:38.164382 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0315 00:07:38.165279 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0315 00:07:38.166142 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0315 00:07:38.167053 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0315 00:07:38.167920 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:07:38.168813 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0315 00:07:38.169679 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0315 00:07:38.175549 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0315 00:07:38.180905 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0315 00:07:38.181800 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0315 00:07:38.182808 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0315 00:07:38.184797 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0315 00:07:38.185714 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0315 00:07:38.186634 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.188884 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.189808 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0315 00:07:38.191019 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0315 00:07:38.192009 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0315 00:07:38.193100 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0315 00:07:38.193984 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0315 00:07:38.194904 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0315 00:07:38.195797 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0315 00:07:38.196677 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0315 00:07:38.197538 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0315 00:07:38.198503 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0315 00:07:38.199526 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0315 00:07:38.203998 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0315 00:07:38.204957 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0315 00:07:38.205944 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight\n",
      "I0315 00:07:38.206979 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.208077 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.208964 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0315 00:07:38.209876 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0315 00:07:38.210767 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0315 00:07:38.211667 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0315 00:07:38.212536 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0315 00:07:38.213460 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0315 00:07:38.214338 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0315 00:07:38.215254 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0315 00:07:38.216124 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0315 00:07:38.217088 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0315 00:07:38.217915 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0315 00:07:38.218925 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0315 00:07:38.225809 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0315 00:07:38.226858 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0315 00:07:38.227830 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.228827 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.229712 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0315 00:07:38.230671 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0315 00:07:38.231661 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0315 00:07:38.232625 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0315 00:07:38.233520 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0315 00:07:38.234493 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0315 00:07:38.235422 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0315 00:07:38.236961 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:07:38.237786 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0315 00:07:38.238969 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0315 00:07:38.239895 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0315 00:07:38.240849 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0315 00:07:38.241770 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0315 00:07:38.242809 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0315 00:07:38.244468 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.245430 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.246461 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0315 00:07:38.248042 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0315 00:07:38.249127 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0315 00:07:38.250093 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0315 00:07:38.251066 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0315 00:07:38.251998 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0315 00:07:38.252902 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0315 00:07:38.253810 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0315 00:07:38.254835 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0315 00:07:38.255827 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0315 00:07:38.257285 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0315 00:07:38.258392 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "I0315 00:07:38.259339 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0315 00:07:38.260375 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0315 00:07:38.261340 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0315 00:07:38.262357 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0315 00:07:38.264007 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0315 00:07:38.265044 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0315 00:07:38.265931 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0315 00:07:38.266949 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0315 00:07:38.267818 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0315 00:07:38.268780 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0315 00:07:38.269653 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0315 00:07:38.270640 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0315 00:07:38.271510 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0315 00:07:38.272684 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0315 00:07:38.273693 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0315 00:07:38.274672 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0315 00:07:38.275563 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0315 00:07:38.276554 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0315 00:07:38.277443 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias\n",
      "I0315 00:07:38.278536 140098058590016 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight\n",
      "I0315 00:07:38.862114 140098058590016 params.py:250] dataset_reader.type = aita_test_reader\n",
      "I0315 00:07:38.864401 140098058590016 params.py:250] dataset_reader.lazy = False\n",
      "I0315 00:07:38.865727 140098058590016 params.py:250] dataset_reader.cache_directory = None\n",
      "I0315 00:07:38.867198 140098058590016 params.py:250] dataset_reader.max_instances = None\n",
      "I0315 00:07:38.868937 140098058590016 params.py:250] dataset_reader.tokenizer.type = pretrained_transformer\n",
      "I0315 00:07:38.870824 140098058590016 params.py:250] dataset_reader.tokenizer.model_name = roberta-base\n",
      "I0315 00:07:38.872325 140098058590016 params.py:250] dataset_reader.tokenizer.add_special_tokens = True\n",
      "I0315 00:07:38.876553 140098058590016 params.py:250] dataset_reader.tokenizer.max_length = None\n",
      "I0315 00:07:38.877839 140098058590016 params.py:250] dataset_reader.tokenizer.stride = 0\n",
      "I0315 00:07:38.878930 140098058590016 params.py:250] dataset_reader.tokenizer.truncation_strategy = longest_first\n",
      "I0315 00:07:38.880028 140098058590016 params.py:250] dataset_reader.tokenizer.calculate_character_offsets = False\n",
      "I0315 00:07:38.882827 140098058590016 params.py:250] dataset_reader.tokenizer.tokenizer_kwargs = None\n",
      "I0315 00:07:39.287175 140098058590016 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0315 00:07:39.289741 140098058590016 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:07:40.099144 140098058590016 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/wfu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0315 00:07:40.101061 140098058590016 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/wfu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0315 00:07:40.230468 140098058590016 params.py:250] dataset_reader.token_indexers.tokens.type = pretrained_transformer\n",
      "I0315 00:07:40.232379 140098058590016 params.py:250] dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0315 00:07:40.233959 140098058590016 params.py:250] dataset_reader.token_indexers.tokens.model_name = roberta-base\n",
      "I0315 00:07:40.235152 140098058590016 params.py:250] dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0315 00:07:40.236097 140098058590016 params.py:250] dataset_reader.token_indexers.tokens.max_length = 128\n",
      "I0315 00:07:40.631659 140098058590016 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0315 00:07:40.634222 140098058590016 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0315 00:07:41.464100 140098058590016 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/wfu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0315 00:07:41.466141 140098058590016 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/wfu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0315 00:07:41.600311 140098058590016 params.py:250] dataset_reader.combine_input_fields = None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73eaf6d245145f4a2abe56fee8c7fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 00:07:41.708446 140098058590016 test_reader.py:60] Label Initial Counts\n",
      "I0315 00:07:41.753761 140098058590016 test_reader.py:61] NTA    35\n",
      "YTA    21\n",
      "NAH     2\n",
      "Name: label, dtype: int64\n",
      "I0315 00:07:41.757051 140098058590016 test_reader.py:63] Resampling labels, since resample_labels was set to true.\n",
      "I0315 00:07:41.819744 140098058590016 test_reader.py:77] New label sampling is:\n",
      "I0315 00:07:41.824149 140098058590016 test_reader.py:78] NTA    35\n",
      "YTA    35\n",
      "NAH    35\n",
      "Name: label, dtype: int64\n",
      "W0315 00:07:41.864996 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (27 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:41.912781 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (7 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:41.936745 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:41.949283 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (7 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:41.988517 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (18 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.079958 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.124988 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.143328 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (17 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.173012 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (17 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.191616 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (20 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.210627 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.235324 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.255323 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.266004 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.289052 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.300500 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (17 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.312207 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (28 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.354633 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.363264 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.375765 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.385953 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.395110 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.404298 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.414710 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.426922 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.436515 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.445647 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.456202 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.465982 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.476004 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.484911 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.498714 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0315 00:07:42.507775 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.521335 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0315 00:07:42.534115 140098058590016 tokenization_utils.py:1145] Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "archive = load_archive('/tmp/roberta-base.tar.gz')\n",
    "predictor = AITAClassifier.from_archive(archive)\n",
    "validation_instances = predictor._dataset_reader.read('../data/aita-tiny-test.pkl')\n",
    "gradients, outputs = predictor.get_gradients(validation_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
