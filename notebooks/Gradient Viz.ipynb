{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Visualizing-Gradients-and-Attention\" data-toc-modified-id=\"Visualizing-Gradients-and-Attention-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Visualizing Gradients and Attention</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-in-Model\" data-toc-modified-id=\"Loading-in-Model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Loading in Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Gradients and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T19:34:26.719140Z",
     "start_time": "2020-03-17T19:34:23.527916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:24.983000 140578606368576 file_utils.py:38] PyTorch version 1.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "from aita.AITAPredictor import AITAClassifier\n",
    "from aita.AITAReader import AITATestReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.models.archival import load_archive \n",
    "\n",
    "import numpy as np\n",
    "from attention_viz import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T19:34:58.658875Z",
     "start_time": "2020-03-17T19:34:26.723526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:26.736309 140578606368576 archival.py:152] loading archive file /tmp/roberta-twoclass-tune.tar.gz\n",
      "I0317 12:34:26.739576 140578606368576 archival.py:161] extracting archive file /tmp/roberta-twoclass-tune.tar.gz to temp dir /tmp/tmpjt5r1bwo\n",
      "I0317 12:34:35.555477 140578606368576 params.py:250] type = from_instances\n",
      "I0317 12:34:35.559349 140578606368576 vocabulary.py:293] Loading token dictionary from /tmp/tmpjt5r1bwo/vocabulary.\n",
      "I0317 12:34:35.565595 140578606368576 params.py:250] model.type = basic_classifier\n",
      "I0317 12:34:35.567782 140578606368576 params.py:250] model.regularizer = None\n",
      "I0317 12:34:35.569690 140578606368576 params.py:250] model.text_field_embedder.type = basic\n",
      "I0317 12:34:35.572562 140578606368576 params.py:250] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer\n",
      "I0317 12:34:35.575366 140578606368576 params.py:250] model.text_field_embedder.token_embedders.tokens.model_name = roberta-base\n",
      "I0317 12:34:35.576611 140578606368576 params.py:250] model.text_field_embedder.token_embedders.tokens.max_length = 442\n",
      "I0317 12:34:36.001833 140578606368576 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0317 12:34:36.004895 140578606368576 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0317 12:34:36.422461 140578606368576 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /home/wfu/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0317 12:34:52.729996 140578606368576 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0317 12:34:52.734489 140578606368576 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0317 12:34:53.573709 140578606368576 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/wfu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0317 12:34:53.575141 140578606368576 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/wfu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0317 12:34:53.900671 140578606368576 params.py:250] model.seq2vec_encoder.type = cls_pooler\n",
      "I0317 12:34:53.902373 140578606368576 params.py:250] model.seq2vec_encoder.embedding_dim = 768\n",
      "I0317 12:34:53.903827 140578606368576 params.py:250] model.seq2vec_encoder.cls_is_last_token = False\n",
      "I0317 12:34:53.905019 140578606368576 params.py:250] model.seq2seq_encoder = None\n",
      "I0317 12:34:53.907399 140578606368576 params.py:250] model.feedforward.input_dim = 768\n",
      "I0317 12:34:53.910339 140578606368576 params.py:250] model.feedforward.num_layers = 2\n",
      "I0317 12:34:53.911504 140578606368576 params.py:250] model.feedforward.hidden_dims = [768, 200]\n",
      "I0317 12:34:53.913020 140578606368576 params.py:250] model.feedforward.activations = tanh\n",
      "I0317 12:34:53.914849 140578606368576 params.py:250] type = tanh\n",
      "I0317 12:34:53.918280 140578606368576 params.py:250] model.feedforward.dropout = 0.0\n",
      "I0317 12:34:53.933911 140578606368576 params.py:250] model.dropout = 0.11580221655434887\n",
      "I0317 12:34:53.935415 140578606368576 params.py:250] model.num_labels = None\n",
      "I0317 12:34:53.936832 140578606368576 params.py:250] model.label_namespace = labels\n",
      "I0317 12:34:53.938286 140578606368576 params.py:250] model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7fda491524e0>\n",
      "I0317 12:34:53.940568 140578606368576 initializers.py:410] Initializing parameters\n",
      "I0317 12:34:53.945528 140578606368576 initializers.py:429] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0317 12:34:53.947316 140578606368576 initializers.py:435]    _classification_layer.bias\n",
      "I0317 12:34:53.948645 140578606368576 initializers.py:435]    _classification_layer.weight\n",
      "I0317 12:34:53.950047 140578606368576 initializers.py:435]    _feedforward._linear_layers.0.bias\n",
      "I0317 12:34:53.953007 140578606368576 initializers.py:435]    _feedforward._linear_layers.0.weight\n",
      "I0317 12:34:53.954078 140578606368576 initializers.py:435]    _feedforward._linear_layers.1.bias\n",
      "I0317 12:34:53.955265 140578606368576 initializers.py:435]    _feedforward._linear_layers.1.weight\n",
      "I0317 12:34:53.956452 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias\n",
      "I0317 12:34:53.959309 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight\n",
      "I0317 12:34:53.960463 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight\n",
      "I0317 12:34:53.961614 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0317 12:34:53.962731 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight\n",
      "I0317 12:34:53.964103 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0317 12:34:53.965604 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:53.967173 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0317 12:34:53.968676 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0317 12:34:53.970000 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0317 12:34:53.971133 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0317 12:34:53.972350 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0317 12:34:53.973579 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0317 12:34:53.974713 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0317 12:34:53.976848 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0317 12:34:53.982706 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0317 12:34:53.983614 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0317 12:34:53.984630 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0317 12:34:53.986523 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0317 12:34:53.987819 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0317 12:34:53.990178 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0317 12:34:53.991551 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0317 12:34:53.992785 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0317 12:34:53.994113 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0317 12:34:53.995315 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0317 12:34:53.996434 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0317 12:34:53.997532 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0317 12:34:53.998619 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0317 12:34:53.999717 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0317 12:34:54.000951 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0317 12:34:54.006071 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0317 12:34:54.007312 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0317 12:34:54.009087 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0317 12:34:54.010121 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0317 12:34:54.011039 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0317 12:34:54.011967 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0317 12:34:54.012871 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0317 12:34:54.015341 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.016202 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.017126 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0317 12:34:54.018021 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0317 12:34:54.018934 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0317 12:34:54.019809 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0317 12:34:54.021076 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0317 12:34:54.023699 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0317 12:34:54.024614 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0317 12:34:54.026187 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0317 12:34:54.027063 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "I0317 12:34:54.027978 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0317 12:34:54.028877 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0317 12:34:54.029784 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0317 12:34:54.030659 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0317 12:34:54.031563 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0317 12:34:54.032454 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.033355 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.034241 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0317 12:34:54.035139 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:54.036009 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0317 12:34:54.040595 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0317 12:34:54.041994 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0317 12:34:54.042854 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0317 12:34:54.043731 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0317 12:34:54.044704 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0317 12:34:54.045706 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0317 12:34:54.047898 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0317 12:34:54.049431 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0317 12:34:54.050334 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0317 12:34:54.051253 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0317 12:34:54.052139 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0317 12:34:54.053060 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.053935 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.056156 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0317 12:34:54.057178 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0317 12:34:54.058055 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0317 12:34:54.059077 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0317 12:34:54.059947 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0317 12:34:54.060883 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0317 12:34:54.061784 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0317 12:34:54.062681 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0317 12:34:54.063556 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0317 12:34:54.064500 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0317 12:34:54.065481 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0317 12:34:54.066606 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0317 12:34:54.067495 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0317 12:34:54.068468 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0317 12:34:54.069417 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.070377 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.071269 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0317 12:34:54.072171 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0317 12:34:54.073106 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0317 12:34:54.074059 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0317 12:34:54.075017 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0317 12:34:54.076046 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0317 12:34:54.077002 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0317 12:34:54.078121 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0317 12:34:54.086640 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0317 12:34:54.087617 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0317 12:34:54.088587 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0317 12:34:54.089570 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0317 12:34:54.090595 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0317 12:34:54.092904 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0317 12:34:54.093787 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.094704 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.095589 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0317 12:34:54.096542 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0317 12:34:54.097509 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0317 12:34:54.098462 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:54.099348 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0317 12:34:54.100274 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0317 12:34:54.101204 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0317 12:34:54.102153 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0317 12:34:54.103039 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0317 12:34:54.103942 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0317 12:34:54.108961 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0317 12:34:54.109893 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0317 12:34:54.111600 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0317 12:34:54.112607 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0317 12:34:54.114974 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.115887 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.116777 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0317 12:34:54.117667 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0317 12:34:54.118534 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0317 12:34:54.119415 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0317 12:34:54.120329 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0317 12:34:54.121282 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0317 12:34:54.122230 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0317 12:34:54.123181 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0317 12:34:54.124080 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0317 12:34:54.125001 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0317 12:34:54.125856 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0317 12:34:54.126729 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0317 12:34:54.127576 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0317 12:34:54.128484 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight\n",
      "I0317 12:34:54.129441 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.130406 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.131295 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0317 12:34:54.132253 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0317 12:34:54.133178 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0317 12:34:54.134098 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0317 12:34:54.135011 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0317 12:34:54.135919 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0317 12:34:54.136844 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0317 12:34:54.137773 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0317 12:34:54.138669 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0317 12:34:54.139605 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0317 12:34:54.140547 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0317 12:34:54.141521 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0317 12:34:54.142343 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0317 12:34:54.143205 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0317 12:34:54.144053 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.144943 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.145781 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0317 12:34:54.147246 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0317 12:34:54.148136 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0317 12:34:54.149028 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0317 12:34:54.149890 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0317 12:34:54.150737 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:54.151584 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0317 12:34:54.152467 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0317 12:34:54.153523 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0317 12:34:54.164070 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0317 12:34:54.165508 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0317 12:34:54.166628 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0317 12:34:54.167545 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0317 12:34:54.168472 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0317 12:34:54.173185 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.174450 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.175550 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0317 12:34:54.176507 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0317 12:34:54.177612 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0317 12:34:54.179158 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0317 12:34:54.180220 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0317 12:34:54.181235 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0317 12:34:54.182808 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0317 12:34:54.183705 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0317 12:34:54.184667 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0317 12:34:54.185528 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0317 12:34:54.186553 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0317 12:34:54.187521 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "I0317 12:34:54.188531 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0317 12:34:54.193055 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0317 12:34:54.193948 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0317 12:34:54.194917 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0317 12:34:54.195925 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0317 12:34:54.196930 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0317 12:34:54.197947 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0317 12:34:54.198917 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0317 12:34:54.200099 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0317 12:34:54.201146 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0317 12:34:54.202110 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0317 12:34:54.203202 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0317 12:34:54.204666 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0317 12:34:54.205855 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0317 12:34:54.207076 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0317 12:34:54.208421 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0317 12:34:54.209755 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0317 12:34:54.211008 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0317 12:34:54.212346 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias\n",
      "I0317 12:34:54.219601 140578606368576 initializers.py:435]    _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight\n",
      "I0317 12:34:55.233702 140578606368576 params.py:250] dataset_reader.type = aita_transformer_reader\n",
      "I0317 12:34:55.237217 140578606368576 params.py:250] dataset_reader.lazy = False\n",
      "I0317 12:34:55.238604 140578606368576 params.py:250] dataset_reader.cache_directory = None\n",
      "I0317 12:34:55.239919 140578606368576 params.py:250] dataset_reader.max_instances = None\n",
      "I0317 12:34:55.241661 140578606368576 params.py:250] dataset_reader.tokenizer.type = pretrained_transformer\n",
      "I0317 12:34:55.243297 140578606368576 params.py:250] dataset_reader.tokenizer.model_name = roberta-base\n",
      "I0317 12:34:55.244598 140578606368576 params.py:250] dataset_reader.tokenizer.add_special_tokens = True\n",
      "I0317 12:34:55.245890 140578606368576 params.py:250] dataset_reader.tokenizer.max_length = 442\n",
      "I0317 12:34:55.247321 140578606368576 params.py:250] dataset_reader.tokenizer.stride = 0\n",
      "I0317 12:34:55.248574 140578606368576 params.py:250] dataset_reader.tokenizer.truncation_strategy = longest_first\n",
      "I0317 12:34:55.249740 140578606368576 params.py:250] dataset_reader.tokenizer.calculate_character_offsets = False\n",
      "I0317 12:34:55.250966 140578606368576 params.py:250] dataset_reader.tokenizer.tokenizer_kwargs = None\n",
      "I0317 12:34:55.661887 140578606368576 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:55.664920 140578606368576 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0317 12:34:56.458260 140578606368576 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/wfu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0317 12:34:56.460110 140578606368576 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/wfu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0317 12:34:56.592406 140578606368576 params.py:250] dataset_reader.token_indexers.tokens.type = pretrained_transformer\n",
      "I0317 12:34:56.594547 140578606368576 params.py:250] dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0317 12:34:56.595852 140578606368576 params.py:250] dataset_reader.token_indexers.tokens.model_name = roberta-base\n",
      "I0317 12:34:56.597104 140578606368576 params.py:250] dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0317 12:34:56.598435 140578606368576 params.py:250] dataset_reader.token_indexers.tokens.max_length = 442\n",
      "I0317 12:34:57.026731 140578606368576 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0317 12:34:57.029035 140578606368576 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0317 12:34:57.916561 140578606368576 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/wfu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0317 12:34:57.918156 140578606368576 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/wfu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0317 12:34:58.051919 140578606368576 params.py:250] dataset_reader.combine_input_fields = None\n",
      "I0317 12:34:58.053470 140578606368576 params.py:250] dataset_reader.two_classes = True\n",
      "I0317 12:34:58.054824 140578606368576 params.py:250] dataset_reader.max_samples = -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963e18e40f1742c5886fb67dc597f5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 12:34:58.113169 140578606368576 AITAReader.py:238] Label Initial Counts\n",
      "I0317 12:34:58.122203 140578606368576 AITAReader.py:239] NTA    35\n",
      "YTA    21\n",
      "NAH     2\n",
      "Name: label, dtype: int64\n",
      "I0317 12:34:58.124751 140578606368576 AITAReader.py:243] Simplifying dataset to only use 2 classes\n",
      "I0317 12:34:58.125949 140578606368576 AITAReader.py:244] Using classes: ['NTA', 'YTA']\n",
      "I0317 12:34:58.129499 140578606368576 AITAReader.py:247] Resampling labels, since resample_labels was set to true.\n",
      "I0317 12:34:58.175658 140578606368576 AITAReader.py:260] New label sampling is:\n",
      "I0317 12:34:58.179147 140578606368576 AITAReader.py:261] NTA    35\n",
      "YTA    35\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "archive = load_archive('/tmp/roberta-twoclass-tune.tar.gz')\n",
    "predictor = AITAClassifier.from_archive(archive)\n",
    "validation_instances = predictor._dataset_reader.read('../data/aita-tiny-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T19:34:58.674133Z",
     "start_time": "2020-03-17T19:34:58.663678Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gradients_and_tokens(validation_instance):\n",
    "    tokens = [str(a).replace(\"$\", \"[DOLLAR]\") for a in validation_instance['tokens']]    \n",
    "    gradients, outputs = predictor.get_gradients([validation_instance])\n",
    "    gradient = gradients['grad_input_1'][0]\n",
    "    gradient_norm = np.power(gradient.sum(axis=1), 2)\n",
    "    gradient_norm /= gradient_norm.sum()\n",
    "    gradient_norm[gradient_norm < 0.0001] = 0.\n",
    "    gradient_norm = gradient_norm / gradient_norm.max() * 200\n",
    "    return gradient_norm, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:35:20.241738Z",
     "start_time": "2020-03-17T20:35:16.798866Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_grad, plot_tokens = get_gradients_and_tokens(validation_instances[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:35:20.257910Z",
     "start_time": "2020-03-17T20:35:20.248721Z"
    }
   },
   "outputs": [],
   "source": [
    "word_num = len(plot_tokens)\n",
    "attention = plot_grad[:word_num]\n",
    "color = 'red'\n",
    "generate(plot_tokens, attention, \"sample.tex\", color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T19:51:00.336538Z",
     "start_time": "2020-03-17T19:51:00.328642Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
