{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:34:09.987612Z",
     "start_time": "2020-03-15T16:34:09.250145Z"
    }
   },
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "from typing import List, Sequence, Iterable, Tuple, Dict\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, ArrayField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer\n",
    "from allennlp.data.tokenizers import PretrainedTransformerTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:29:54.287437Z",
     "start_time": "2020-03-15T16:29:54.262766Z"
    }
   },
   "outputs": [],
   "source": [
    "@DatasetReader.register(\"aita_transformer_reader\")\n",
    "class AITATestReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads a file from the Stanford Natural Language Inference (SNLI) dataset.  This data is\n",
    "    formatted as jsonl, one json-formatted instance per line.  The keys in the data are\n",
    "    \"gold_label\", \"sentence1\", and \"sentence2\".  We convert these keys into fields named \"label\",\n",
    "    \"premise\" and \"hypothesis\", along with a metadata field containing the tokenized strings of the\n",
    "    premise and hypothesis.\n",
    "    # Parameters\n",
    "    tokenizer : `Tokenizer`, optional (default=`SpacyTokenizer()`)\n",
    "        We use this `Tokenizer` for both the premise and the hypothesis.  See :class:`Tokenizer`.\n",
    "    token_indexers : `Dict[str, TokenIndexer]`, optional (default=`{\"tokens\": SingleIdTokenIndexer()}`)\n",
    "        We similarly use this for both the premise and the hypothesis.  See :class:`TokenIndexer`.\n",
    "    combine_input_fields : `bool`, optional\n",
    "            (default=`isinstance(tokenizer, PretrainedTransformerTokenizer)`)\n",
    "        If False, represent the premise and the hypothesis as separate fields in the instance.\n",
    "        If True, tokenize them together using `tokenizer.tokenize_sentence_pair()`\n",
    "        and provide a single `tokens` field in the instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Tokenizer = None,\n",
    "        token_indexers: Dict[str, TokenIndexer] = None,\n",
    "        combine_input_fields: bool = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._tokenizer = tokenizer or SpacyTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        if combine_input_fields is not None:\n",
    "            self._combine_input_fields = combine_input_fields\n",
    "        else:\n",
    "            self._combine_input_fields = isinstance(self._tokenizer, PretrainedTransformerTokenizer)\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str):\n",
    "        # if `file_path` is a URL, redirect to the cache\n",
    "        file_path = cached_path(file_path)\n",
    "        df = pd.read_pickle(file_path)\n",
    "        logger.info(\"Label Initial Counts\")\n",
    "        logger.info(df.label.value_counts())\n",
    "\n",
    "        logger.info(\"Resampling labels, since resample_labels\"\n",
    "            \" was set to true.\")\n",
    "        labels = list(df.label.unique())\n",
    "        label_dataframes = []\n",
    "        for label in labels:\n",
    "            label_dataframes.append(df[df.label == label])\n",
    "        label_counts = [len(x) for x in label_dataframes]\n",
    "        largest_label = max(label_counts)\n",
    "        df = pd.concat([\n",
    "            resample(label_df,\n",
    "                replace=True,\n",
    "                n_samples=largest_label,\n",
    "                random_state=420)\n",
    "            for label_df in label_dataframes])\n",
    "        logger.info(\"New label sampling is:\")\n",
    "        logger.info(df.label.value_counts())\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            yield self.text_to_instance(row.title, row.selftext, row.label)\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(\n",
    "        self,  # type: ignore\n",
    "        title: str,\n",
    "        post: str,\n",
    "        label: str = None,\n",
    "    ) -> Instance:\n",
    "\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokens = self._tokenizer.tokenize_sentence_pair(title, post)\n",
    "        fields[\"tokens\"] = TextField(tokens, self._token_indexers)\n",
    "\n",
    "        if label:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:34:12.855961Z",
     "start_time": "2020-03-15T16:34:12.828172Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/aita-tiny-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:41:19.520280Z",
     "start_time": "2020-03-15T16:41:17.839094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 09:41:18.396000 140644183689024 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/wfu/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "I0315 09:41:18.399602 140644183689024 configuration_utils.py:290] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0315 09:41:19.378808 140644183689024 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/wfu/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0315 09:41:19.381175 140644183689024 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/wfu/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PretrainedTransformerTokenizer('roberta-base', max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:45:33.936986Z",
     "start_time": "2020-03-15T16:45:33.931008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method tokenize_sentence_pair in module allennlp.data.tokenizers.pretrained_transformer_tokenizer:\n",
      "\n",
      "tokenize_sentence_pair(sentence_1:str, sentence_2:str) -> List[allennlp.data.tokenizers.token.Token] method of allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer instance\n",
      "    This methods properly handles a pair of sentences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.tokenize_sentence_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:47:33.951094Z",
     "start_time": "2020-03-15T16:47:33.930253Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokenize() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e1550424da32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tokenize() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Hi \" * 1000, \"Test \" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
