{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:38:10.427868Z",
     "start_time": "2020-03-14T21:38:10.413653Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from subreddit_frequency import load_dataframe_from_jsonl\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "sns.set('paper')\n",
    "\n",
    "from ipywidgets import interact\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:28:02.243424Z",
     "start_time": "2020-03-14T21:28:02.238184Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = Path.cwd().parent / \"aita\" / \"aita-train.pkl\"\n",
    "test_path = Path.cwd().parent / \"aita\" / \"aita-test.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:28:03.568739Z",
     "start_time": "2020-03-14T21:28:03.316215Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_df = pd.read_pickle(train_path)\n",
    "test_dataset_df = pd.read_pickle(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one-hot no embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T15:46:23.896076Z",
     "start_time": "2020-03-13T15:46:23.759587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10223"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all of the words from our training set and count frequencies\n",
    "word_counts = defaultdict(int)\n",
    "for post in train_dataset_df.selftext.iteritems():\n",
    "    text = post[1].strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T15:46:24.972258Z",
     "start_time": "2020-03-13T15:46:24.964380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create token mapping\n",
    "word_to_token = dict()\n",
    "token_to_word = dict()\n",
    "word_to_token['<UNK>'] = 0\n",
    "token_to_word[0] = '<UNK>'\n",
    "i = 1\n",
    "for word, count in word_counts.items():\n",
    "    if count < 7:\n",
    "        continue\n",
    "    word_to_token[word] = i\n",
    "    token_to_word[i] = word\n",
    "    i += 1\n",
    "len(word_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T21:45:23.327480Z",
     "start_time": "2020-03-13T21:45:23.323288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize and detokenize\n",
    "def tokenize_post(post):\n",
    "    text = post.strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    output = []\n",
    "    for word in words:\n",
    "        output.append(word_to_token.get(word, 0))\n",
    "    return torch.eye(len(word_to_token))[np.array(output)].sum(axis=0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:40:39.785900Z",
     "start_time": "2020-03-13T22:40:35.593024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize reviews in train dataset\n",
    "train_dataset_df['tokenized_selftext'] = train_dataset_df.selftext.apply(tokenize_post)\n",
    "test_dataset_df['tokenized_selftext'] = test_dataset_df.selftext.apply(tokenize_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:28:29.210201Z",
     "start_time": "2020-03-14T21:28:09.643030Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        token = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[token] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:28:31.762229Z",
     "start_time": "2020-03-14T21:28:31.754583Z"
    }
   },
   "outputs": [],
   "source": [
    "def selftext_to_glove(text, embedding_size=300):\n",
    "    embeddings = [\n",
    "        embeddings_dict[word.lower().strip()]\n",
    "        for word in text.split()\n",
    "        if word.lower().strip() in embeddings_dict\n",
    "    ]\n",
    "    if embeddings:\n",
    "        glove_embeddings = np.stack(embeddings).mean(axis=0)\n",
    "    else:\n",
    "        glove_embeddings = np.zeros(embedding_size)\n",
    "    return glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:28:40.167129Z",
     "start_time": "2020-03-14T21:28:33.635681Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_df['selftext_glove_300'] = train_dataset_df.selftext.apply(selftext_to_glove)\n",
    "test_dataset_df['selftext_glove_300'] = test_dataset_df.selftext.apply(selftext_to_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Feed Forward No Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:28:52.288723Z",
     "start_time": "2020-03-14T21:28:52.278824Z"
    }
   },
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.output_size = output_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:37:30.217905Z",
     "start_time": "2020-03-14T21:37:30.201841Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_features_and_ys(df, features):\n",
    "    feature_df = df[features]\n",
    "    xs = feature_df.apply(\n",
    "        lambda x : np.hstack([np.array(a) for a in x]), axis=1\n",
    "    ).tolist()\n",
    "    label_index = sorted(train_dataset_df.label.unique())\n",
    "    ys = np.array(list(map(label_index.index, df.label.to_list())))\n",
    "    return torch.Tensor(xs), torch.LongTensor(ys)\n",
    "\n",
    "def train_model(model, xs, ys, epochs=10, batch_size=10):\n",
    "    model = model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    dataset = torch_data.TensorDataset(xs, ys)\n",
    "    loader = torch_data.DataLoader(dataset, \n",
    "               batch_size=batch_size,\n",
    "               shuffle=True)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for context, label in loader:\n",
    "            context = context.to(device)\n",
    "            label = label.to(device)\n",
    "            opt.zero_grad()\n",
    "            # Get predictions\n",
    "            outputs = model(context)\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss\n",
    "        print(f\"EPOCH {epoch} LOSS = {epoch_loss}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:36:04.581426Z",
     "start_time": "2020-03-14T21:35:21.642550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 LOSS = 2136.147216796875\n",
      "EPOCH 1 LOSS = 2111.3388671875\n",
      "EPOCH 2 LOSS = 2098.670166015625\n",
      "EPOCH 3 LOSS = 2090.91650390625\n",
      "EPOCH 4 LOSS = 2082.97412109375\n",
      "EPOCH 5 LOSS = 2080.221923828125\n",
      "EPOCH 6 LOSS = 2075.304443359375\n",
      "EPOCH 7 LOSS = 2069.415771484375\n",
      "EPOCH 8 LOSS = 2068.8681640625\n",
      "EPOCH 9 LOSS = 2063.454833984375\n"
     ]
    }
   ],
   "source": [
    "model = Feedforward(300, 512, len(train_dataset_df.label.unique()))\n",
    "xs, ys = build_features_and_ys(train_dataset_df, ['selftext_glove_300'])\n",
    "trained_model = train_model(model, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:35:36.605234Z",
     "start_time": "2020-03-13T22:35:36.603226Z"
    }
   },
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:39:20.705749Z",
     "start_time": "2020-03-14T21:39:20.696342Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_accuracy(model, xs, ys):\n",
    "    model = model.to(device)\n",
    "    xs = xs.to(device)\n",
    "    print(np.mean((model(xs).to(\"cpu\").argmax(axis=1) == ys).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:39:21.562045Z",
     "start_time": "2020-03-14T21:39:21.280662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5758957654723127\n"
     ]
    }
   ],
   "source": [
    "test_xs, test_ys = build_features_and_ys(test_dataset_df, ['selftext_glove_300'])\n",
    "get_model_accuracy(trained_model, test_xs, test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
