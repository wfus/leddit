{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T19:15:10.805262Z",
     "start_time": "2020-03-14T19:15:09.049135Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from subreddit_frequency import load_dataframe_from_jsonl\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "sns.set('paper')\n",
    "\n",
    "from ipywidgets import interact\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T19:15:31.297331Z",
     "start_time": "2020-03-14T19:15:31.294101Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = Path.cwd().parent / \"aita\" / \"aita-train.pkl\"\n",
    "test_path = Path.cwd().parent / \"aita\" / \"aita-test.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T19:17:06.910179Z",
     "start_time": "2020-03-14T19:17:06.563876Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_df = pd.read_pickle(train_path)\n",
    "test_dataset_df = pd.read_pickle(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one-hot no embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T15:46:23.896076Z",
     "start_time": "2020-03-13T15:46:23.759587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10223"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all of the words from our training set and count frequencies\n",
    "word_counts = defaultdict(int)\n",
    "for post in train_dataset_df.selftext.iteritems():\n",
    "    text = post[1].strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T15:46:24.972258Z",
     "start_time": "2020-03-13T15:46:24.964380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create token mapping\n",
    "word_to_token = dict()\n",
    "token_to_word = dict()\n",
    "word_to_token['<UNK>'] = 0\n",
    "token_to_word[0] = '<UNK>'\n",
    "i = 1\n",
    "for word, count in word_counts.items():\n",
    "    if count < 7:\n",
    "        continue\n",
    "    word_to_token[word] = i\n",
    "    token_to_word[i] = word\n",
    "    i += 1\n",
    "len(word_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T21:45:23.327480Z",
     "start_time": "2020-03-13T21:45:23.323288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize and detokenize\n",
    "def tokenize_post(post):\n",
    "    text = post.strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    output = []\n",
    "    for word in words:\n",
    "        output.append(word_to_token.get(word, 0))\n",
    "    return torch.eye(len(word_to_token))[np.array(output)].sum(axis=0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:40:39.785900Z",
     "start_time": "2020-03-13T22:40:35.593024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize reviews in train dataset\n",
    "train_dataset_df['tokenized_selftext'] = train_dataset_df.selftext.apply(tokenize_post)\n",
    "test_dataset_df['tokenized_selftext'] = test_dataset_df.selftext.apply(tokenize_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:00:44.371106Z",
     "start_time": "2020-03-14T20:00:18.539771Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        token = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[token] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:04:34.570306Z",
     "start_time": "2020-03-14T20:04:34.566194Z"
    }
   },
   "outputs": [],
   "source": [
    "def selftext_to_glove(text, embedding_size=300):\n",
    "    embeddings = [\n",
    "        embeddings_dict[word.lower().strip()]\n",
    "        for word in text.split()\n",
    "        if word.lower().strip() in embeddings_dict\n",
    "    ]\n",
    "    if embeddings:\n",
    "        glove_embeddings = np.stack(embeddings).mean(axis=0)\n",
    "    else:\n",
    "        glove_embeddings = np.zeros(embedding_size)\n",
    "    return glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:07:58.095414Z",
     "start_time": "2020-03-14T20:07:50.356260Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_df['selftext_glove_300'] = train_dataset_df.selftext.apply(selftext_to_glove)\n",
    "test_dataset_df['selftext_glove_300'] = test_dataset_df.selftext.apply(selftext_to_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Feed Forward No Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:05:14.310936Z",
     "start_time": "2020-03-14T20:05:14.305588Z"
    }
   },
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.output_size = output_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:05:14.834746Z",
     "start_time": "2020-03-14T20:05:14.826335Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_features_and_ys(df, features):\n",
    "    feature_df = df[features]\n",
    "    xs = feature_df.apply(\n",
    "        lambda x : np.hstack([np.array(a) for a in x]), axis=1\n",
    "    ).tolist()\n",
    "    label_index = sorted(train_dataset_df.label.unique())\n",
    "    ys = np.array(list(map(label_index.index, df.label.to_list())))\n",
    "    return torch.Tensor(xs), torch.LongTensor(ys)\n",
    "\n",
    "def train_model(model, xs, ys, epochs=10, batch_size=10):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    dataset = torch_data.TensorDataset(xs, ys)\n",
    "    loader = torch_data.DataLoader(dataset, \n",
    "               batch_size=batch_size,\n",
    "               shuffle=True)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for context, label in loader:\n",
    "            opt.zero_grad()\n",
    "            # Get predictions\n",
    "            outputs = model(context)\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss\n",
    "        print(f\"EPOCH {epoch} LOSS = {epoch_loss}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:06:33.030441Z",
     "start_time": "2020-03-14T20:05:38.719727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 LOSS = 2137.155029296875\n",
      "EPOCH 1 LOSS = 2106.844970703125\n",
      "EPOCH 2 LOSS = 2097.70751953125\n",
      "EPOCH 3 LOSS = 2090.734375\n",
      "EPOCH 4 LOSS = 2083.0361328125\n",
      "EPOCH 5 LOSS = 2081.213623046875\n",
      "EPOCH 6 LOSS = 2080.346435546875\n",
      "EPOCH 7 LOSS = 2074.66748046875\n",
      "EPOCH 8 LOSS = 2069.96923828125\n",
      "EPOCH 9 LOSS = 2068.792724609375\n"
     ]
    }
   ],
   "source": [
    "model = Feedforward(300, 512, len(train_dataset_df.label.unique()))\n",
    "xs, ys = build_features_and_ys(train_dataset_df, ['selftext_glove_300'])\n",
    "trained_model = train_model(model, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:35:36.605234Z",
     "start_time": "2020-03-13T22:35:36.603226Z"
    }
   },
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:06:45.152987Z",
     "start_time": "2020-03-14T20:06:45.149851Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_accuracy(model, xs, ys):\n",
    "    print(np.mean((model(xs).argmax(axis=1) == ys).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T20:08:04.843909Z",
     "start_time": "2020-03-14T20:08:04.464800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5801302931596091\n"
     ]
    }
   ],
   "source": [
    "test_xs, test_ys = build_features_and_ys(test_dataset_df, ['selftext_glove_300'])\n",
    "get_model_accuracy(trained_model, test_xs, test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal3.8",
   "language": "python",
   "name": "normal3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
