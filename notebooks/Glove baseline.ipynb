{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T15:46:23.896076Z",
     "start_time": "2020-03-13T15:46:23.759587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10223"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all of the words from our training set and count frequencies\n",
    "word_counts = defaultdict(int)\n",
    "for post in train_dataset_df.selftext.iteritems():\n",
    "    text = post[1].strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T15:46:24.972258Z",
     "start_time": "2020-03-13T15:46:24.964380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create token mapping\n",
    "word_to_token = dict()\n",
    "token_to_word = dict()\n",
    "word_to_token['<UNK>'] = 0\n",
    "token_to_word[0] = '<UNK>'\n",
    "i = 1\n",
    "for word, count in word_counts.items():\n",
    "    if count < 7:\n",
    "        continue\n",
    "    word_to_token[word] = i\n",
    "    token_to_word[i] = word\n",
    "    i += 1\n",
    "len(word_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T21:45:23.327480Z",
     "start_time": "2020-03-13T21:45:23.323288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize and detokenize\n",
    "def tokenize_post(post):\n",
    "    text = post.strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    output = []\n",
    "    for word in words:\n",
    "        output.append(word_to_token.get(word, 0))\n",
    "    return torch.eye(len(word_to_token))[np.array(output)].sum(axis=0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:40:39.785900Z",
     "start_time": "2020-03-13T22:40:35.593024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize reviews in train dataset\n",
    "train_dataset_df['tokenized_selftext'] = train_dataset_df.selftext.apply(tokenize_post)\n",
    "test_dataset_df['tokenized_selftext'] = test_dataset_df.selftext.apply(tokenize_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Feed Forward No Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:34:30.500089Z",
     "start_time": "2020-03-13T22:34:30.494551Z"
    }
   },
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.output_size = output_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T23:18:08.414445Z",
     "start_time": "2020-03-13T23:18:08.405643Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_features_and_ys(df, features):\n",
    "    feature_df = df[features]\n",
    "    xs = feature_df.apply(\n",
    "        lambda x : np.hstack([np.array(a) for a in x]), axis=1\n",
    "    ).tolist()\n",
    "    label_index = sorted(train_dataset_df.label.unique())\n",
    "    ys = np.array(list(map(label_index.index, df.label.to_list())))\n",
    "    return torch.Tensor(xs), torch.LongTensor(ys)\n",
    "\n",
    "def train_model(model, xs, ys, epochs=10, batch_size=10):\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    dataset = torch_data.TensorDataset(xs, ys)\n",
    "    loader = torch_data.DataLoader(dataset, \n",
    "               batch_size=batch_size,\n",
    "               shuffle=True)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for context, label in loader:\n",
    "            opt.zero_grad()\n",
    "            # Get predictions\n",
    "            outputs = model(context)\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss\n",
    "        print(f\"EPOCH {epoch} LOSS = {epoch_loss}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T23:18:15.450716Z",
     "start_time": "2020-03-13T23:18:09.082455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 LOSS = 94.05351257324219\n",
      "EPOCH 1 LOSS = 62.314884185791016\n",
      "EPOCH 2 LOSS = 46.70051574707031\n",
      "EPOCH 3 LOSS = 34.05880355834961\n",
      "EPOCH 4 LOSS = 25.592744827270508\n",
      "EPOCH 5 LOSS = 26.413055419921875\n",
      "EPOCH 6 LOSS = 19.499250411987305\n",
      "EPOCH 7 LOSS = 18.350688934326172\n",
      "EPOCH 8 LOSS = 17.992080688476562\n",
      "EPOCH 9 LOSS = 17.615934371948242\n"
     ]
    }
   ],
   "source": [
    "model = Feedforward(len(word_to_token), 512, len(train_dataset_df.label.unique()))\n",
    "xs, ys = build_features_and_ys(train_dataset_df, ['tokenized_selftext'])\n",
    "trained_model = train_model(model, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:35:36.605234Z",
     "start_time": "2020-03-13T22:35:36.603226Z"
    }
   },
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T23:18:21.691413Z",
     "start_time": "2020-03-13T23:18:21.688452Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_accuracy(model, xs, ys):\n",
    "    print(np.mean((model(xs).argmax(axis=1) == ys).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T23:18:22.159534Z",
     "start_time": "2020-03-13T23:18:22.102308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5660377358490566\n"
     ]
    }
   ],
   "source": [
    "test_xs, test_ys = build_features_and_ys(test_dataset_df, ['tokenized_selftext'])\n",
    "get_model_accuracy(trained_model, test_xs, test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add tokenized text to df\n",
    "\n",
    "# Extract all of the words from our training set and count frequencies\n",
    "word_counts = defaultdict(int)\n",
    "for post in train_dataset_df.selftext.iteritems():\n",
    "    text = post[1].strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "len(word_counts)\n",
    "\n",
    "# Create token mapping\n",
    "word_to_token = dict()\n",
    "token_to_word = dict()\n",
    "word_to_token['<UNK>'] = 0\n",
    "token_to_word[0] = '<UNK>'\n",
    "i = 1\n",
    "for word, count in word_counts.items():\n",
    "    if count < 7:\n",
    "        continue\n",
    "    word_to_token[word] = i\n",
    "    token_to_word[i] = word\n",
    "    i += 1\n",
    "len(word_to_token)\n",
    "\n",
    "# Tokenize and detokenize\n",
    "def tokenize_post(post):\n",
    "    text = post.strip().lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    output = []\n",
    "    for word in words:\n",
    "        output.append(word_to_token.get(word, 0))\n",
    "    return torch.eye(len(word_to_token))[np.array(output)].sum(axis=0)     \n",
    "\n",
    "# Tokenize reviews in train dataset\n",
    "train_dataset_df['tokenized_selftext'] = train_dataset_df.selftext.apply(tokenize_post)\n",
    "test_dataset_df['tokenized_selftext'] = test_dataset_df.selftext.apply(tokenize_post)\n",
    "\n",
    "## Simple Feed Forward No Embeddings\n",
    "\n",
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.output_size = output_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            return output\n",
    "\n",
    "def build_features_and_ys(df, features):\n",
    "    feature_df = df[features]\n",
    "    xs = feature_df.apply(\n",
    "        lambda x : np.hstack([np.array(a) for a in x]), axis=1\n",
    "    ).tolist()\n",
    "    label_index = sorted(train_dataset_df.label.unique())\n",
    "    ys = np.array(list(map(label_index.index, df.label.to_list())))\n",
    "    return torch.Tensor(xs), torch.LongTensor(ys)\n",
    "\n",
    "def train_model(model, xs, ys, epochs=10, batch_size=10):\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    dataset = torch_data.TensorDataset(xs, ys)\n",
    "    loader = torch_data.DataLoader(dataset, \n",
    "               batch_size=batch_size,\n",
    "               shuffle=True)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for context, label in loader:\n",
    "            opt.zero_grad()\n",
    "            # Get predictions\n",
    "            outputs = model(context)\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss\n",
    "        print(f\"EPOCH {epoch} LOSS = {epoch_loss}\")\n",
    "    return model\n",
    "\n",
    "model = Feedforward(len(word_to_token), 512, len(train_dataset_df.label.unique()))\n",
    "xs, ys = build_features_and_ys(train_dataset_df, ['tokenized_selftext'])\n",
    "trained_model = train_model(model, xs, ys)\n",
    "\n",
    "## Validate\n",
    "\n",
    "def get_model_accuracy(model, xs, ys):\n",
    "    print(np.mean((model(xs).argmax(axis=1) == ys).numpy()))\n",
    "\n",
    "test_xs, test_ys = build_features_and_ys(test_dataset_df, ['tokenized_selftext'])\n",
    "get_model_accuracy(trained_model, test_xs, test_ys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal3.8",
   "language": "python",
   "name": "normal3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
